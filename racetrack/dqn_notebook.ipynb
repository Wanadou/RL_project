{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from racetrack_config import *\n",
    "from dqn_v2 import *\n",
    "import pprint\n",
    "\n",
    "print(env.spec)\n",
    "pprint.pprint(env.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = env.action_space\n",
    "observation_space = env.observation_space\n",
    "\n",
    "print(f\"Action space: {action_space}\")\n",
    "print(f\"Observation space: {observation_space}\")\n",
    "\n",
    "# plt.imshow(env.render())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation space corresponds to the information available to the agent at each step. It then chooses an action based on this information. In our case (Kinematics observations), this info is a grid of vehicles_count * n_features where each value corresponds to a feature of a particular vehicule (not including the ego-vehicule ?). This space is stored in a Box object defined by (lower bound, upper bound, (vehicles_count, n_features), numerical_type). Since this observation space is in 2D, we had to flatten this matrix before feeding it into the DQN.\n",
    "\n",
    "The action space is an integer that results from the available actions (longitudinal and/or lateral) and the quantization step (fixed by the parameter actions_per_axis). In the end there are actions_per_axis actions if only one of longiutdinal or lateral is True and actions_per_axis ** 2 if both are true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display, clear_output\n",
    "\n",
    "def eval_agent(agent, env, n_sim=5):\n",
    "    \"\"\"\n",
    "    ** Solution **\n",
    "    \n",
    "    Monte Carlo evaluation of DQN agent.\n",
    "\n",
    "    Repeat n_sim times:\n",
    "        * Run the DQN policy until the environment reaches a terminal state (= one episode)\n",
    "        * Compute the sum of rewards in this episode\n",
    "        * Store the sum of rewards in the episode_rewards array.\n",
    "    \"\"\"\n",
    "    env_copy = deepcopy(env)\n",
    "    episode_rewards = np.zeros(n_sim)\n",
    "    for i in range(n_sim):\n",
    "        state, _ = env_copy.reset()\n",
    "        reward_sum = 0\n",
    "        done = False\n",
    "        while not done: \n",
    "            action = agent.get_action(state, 0)\n",
    "            state, reward, terminated, truncated, _ = env_copy.step(action)\n",
    "            reward_sum += reward\n",
    "            done = terminated or truncated\n",
    "        episode_rewards[i] = reward_sum\n",
    "    return episode_rewards\n",
    "\n",
    "def run_one_episode(env, agent, display=True):\n",
    "    state_result = []\n",
    "    display_env = deepcopy(env)\n",
    "    done = False\n",
    "    state, _ = display_env.reset()\n",
    "\n",
    "    rewards = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.get_action(state, 0)\n",
    "        print(f\"Action : {action}\")\n",
    "        state, reward, done, _, _ = display_env.step(action)\n",
    "        print(f\"State : {state}\")\n",
    "        rewards += reward\n",
    "        if display: \n",
    "            clear_output(wait=True)\n",
    "            plt.imshow(display_env.render())\n",
    "            plt.show()\n",
    "    if display:\n",
    "        display_env.close()\n",
    "    print(f'Episode length {rewards}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a DQN on highway-env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of the DQN agent and training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.8\n",
    "batch_size = 32\n",
    "buffer_capacity = 15_000\n",
    "update_target_every = 32\n",
    "\n",
    "epsilon_start = 0.8\n",
    "decrease_epsilon_factor = 100\n",
    "epsilon_min = 0.02\n",
    "\n",
    "# hidden_size = 256\n",
    "input_channels = 2\n",
    "\n",
    "learning_rate = 5e-4\n",
    "\n",
    "arguments = (env,\n",
    "            gamma,\n",
    "            batch_size,\n",
    "            buffer_capacity,\n",
    "            update_target_every, \n",
    "            epsilon_start, \n",
    "            decrease_epsilon_factor, \n",
    "            epsilon_min,\n",
    "            learning_rate,\n",
    "            input_channels\n",
    "        )\n",
    "\n",
    "print(arguments)\n",
    "\n",
    "agent = DQN_v2(*arguments)\n",
    "\n",
    "def train(env, agent, N_episodes, eval_every=10, reward_threshold=300):\n",
    "    total_time = 0\n",
    "    state, _ = env.reset()\n",
    "    losses = []\n",
    "    training_rewards = []\n",
    "    for ep in range(N_episodes):\n",
    "        done = False\n",
    "        state, _ = env.reset()\n",
    "        while not done: \n",
    "            # print(f\"State shape: {state.shape}\")\n",
    "            action = agent.get_action(state)\n",
    "            # print(f\"Action: {action}\")\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            loss_val = agent.update(state, action, reward, terminated, next_state)\n",
    "\n",
    "            state = next_state\n",
    "            losses.append(loss_val)\n",
    "\n",
    "            done = terminated or truncated\n",
    "            total_time += 1\n",
    "\n",
    "        if ((ep+1)% eval_every == 0):\n",
    "            rewards = eval_agent(agent, env, n_sim=5)\n",
    "            print(\"episode =\", ep+1, \", reward = \", np.mean(rewards))\n",
    "            training_rewards.append(np.mean(rewards))\n",
    "            if np.mean(rewards) >= reward_threshold:\n",
    "                break\n",
    "                \n",
    "    return losses, training_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show initial state of environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the training loop and evaluate agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training loop and save the state of the agent\n",
    "N_episodes = 10\n",
    "losses, training_rewards = train(env, agent, N_episodes)\n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "plt.plot(training_rewards)\n",
    "plt.show()\n",
    "agent.save_state(\"trained_highway_agent_vCNN.pth\")\n",
    "\n",
    "# Or load the state\n",
    "# agent.load_state(\"trained_highway_agent.pth\")\n",
    "\n",
    "# Evaluate the final policy\n",
    "rewards = eval_agent(agent, env, 20)\n",
    "print(\"\")\n",
    "print(\"rewards after training = \", rewards)\n",
    "print(\"mean reward after training = \", np.mean(rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vizualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show trained agent over one episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "# from IPython.display import clear_output\n",
    "\n",
    "run_one_episode(env, agent, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save video of trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML, display, clear_output\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "def run_one_episode_and_save(env, agent, to_display=True, save_video=True, plot_info=True, max_steps=100):\n",
    "    display_env = deepcopy(env)  # Create a deep copy of the environment for isolated manipulation\n",
    "    state, _ = display_env.reset()\n",
    "    frames = []  # Store frames for animation\n",
    "    rewards = 0\n",
    "    step = 0\n",
    "    state_history = []\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        clear_output(wait=True)\n",
    "        # Render the frame and store for video creation\n",
    "        frame = display_env.render()\n",
    "        frames.append(frame)\n",
    "        if to_display:\n",
    "            plt.imshow(frame)\n",
    "            plt.axis('off')  # Hide axes for clean display\n",
    "            plt.show()\n",
    "\n",
    "        # Execute the agent's policy\n",
    "        action = agent.get_action(state, 0)  # Assuming get_action accepts state and epsilon, adjust if different\n",
    "        state, reward, done, _, _ = display_env.step(action)\n",
    "        rewards += reward\n",
    "        step += 1\n",
    "        if plot_info:\n",
    "            state_history.append(state)\n",
    "        if step > max_steps:\n",
    "            done = True\n",
    "        \n",
    "\n",
    "    if to_display:\n",
    "        print(f'Episode length: {rewards}')\n",
    "\n",
    "    # If requested to save the video, proceed with saving\n",
    "    if save_video:\n",
    "        fig, ax = plt.subplots()\n",
    "        img_ax = ax.imshow(frames[0])  # Show the first frame\n",
    "        ax.axis('off')  # Hide axes\n",
    "\n",
    "        def update(frame):\n",
    "            img_ax.set_data(frame)  # Update the image displayed\n",
    "\n",
    "        ani = FuncAnimation(fig, update, frames=frames, repeat=False)\n",
    "\n",
    "        # Save the animation as MP4\n",
    "        video_filename = 'trained_agent_simulation_vCNN.mp4'\n",
    "        ani.save(video_filename, writer='ffmpeg', fps=10)\n",
    "\n",
    "        # Optionally display the animation within the notebook\n",
    "        plt.close(fig)  # Close the figure to prevent it from displaying now\n",
    "        if to_display:\n",
    "            video_tag = f'<video src=\"{video_filename}\" width=\"100%\" controls loop autoplay>'\n",
    "            display(HTML(video_tag))\n",
    "\n",
    "    if to_display:\n",
    "        display_env.close()  # Properly close the environment when done\n",
    "\n",
    "    if plot_info:\n",
    "        # Plot the heatmap of the state values on average over time \n",
    "        state_history = np.array(state_history)\n",
    "        mean_state_history = np.sum(state_history, axis=0) / state_history.shape[0]\n",
    "        presence = mean_state_history[:, :, 0]\n",
    "        # speed = mean_state_history[:, :, 1]\n",
    "        # lane = mean_state_history[:, :, 2]\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.imshow(presence, cmap='hot', interpolation='nearest')\n",
    "        plt.colorbar()\n",
    "        plt.title('Heatmap of presence')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate the trained agent and save the video\n",
    "run_one_episode_and_save(env, agent, to_display=True, save_video=True, plot_info=True, max_steps=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
