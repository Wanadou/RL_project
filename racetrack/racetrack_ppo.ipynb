{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True\n",
    "\n",
    "if True : \n",
    "    n_cpu = 6\n",
    "    batch_size = 64\n",
    "    env = make_vec_env(\"racetrack-v0\", n_envs=n_cpu, vec_env_cls=SubprocVecEnv)\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        policy_kwargs=dict(net_arch=[dict(pi=[256, 256], vf=[256, 256])]),\n",
    "        n_steps=batch_size * 12 // n_cpu,\n",
    "        batch_size=batch_size,\n",
    "        n_epochs=10,\n",
    "        learning_rate=5e-4,\n",
    "        gamma=0.9,\n",
    "        verbose=2,\n",
    "        tensorboard_log=\"racetrack_ppo/\",\n",
    "    )\n",
    "    # Train the model\n",
    "    if TRAIN:\n",
    "        model.learn(total_timesteps=int(1e5))\n",
    "        model.save(\"racetrack_ppo/model\")\n",
    "        del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the algorithm\n",
    "model = PPO.load(\"racetrack_ppo/model\", env=env)\n",
    "\n",
    "env = gym.make(\"racetrack-v0\", render_mode=\"rgb_array\")\n",
    "env = RecordVideo(\n",
    "    env, video_folder=\"racetrack_ppo/videos\", episode_trigger=lambda e: True\n",
    ")\n",
    "env.unwrapped.set_record_video_wrapper(env)\n",
    "\n",
    "for video in range(10):\n",
    "    done = truncated = False\n",
    "    obs, info = env.reset()\n",
    "    while not (done or truncated):\n",
    "        # Predict\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        # Get reward\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        # Render\n",
    "        env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Sterring and Acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"racetrack-v0\", render_mode=\"rgb_array\")\n",
    "\n",
    "config = {\n",
    "    \"observation\": {\n",
    "        \"type\": \"OccupancyGrid\",\n",
    "        \"features\": [\"presence\", \"on_road\"],\n",
    "        \"grid_size\": [[-18, 18], [-18, 18]],\n",
    "        \"grid_step\": [5, 5],\n",
    "        \"as_image\": False,\n",
    "        \"align_to_vehicle_axes\": True\n",
    "    },\n",
    "    \"action\": {\n",
    "        \"type\": \"ContinuousAction\",\n",
    "        \"longitudinal\": True,\n",
    "        \"lateral\": True,\n",
    "        \"steering_range\": [-np.pi / 4, np.pi / 4],  # [rad]\n",
    "        \"acceleration_range\": [-2, 2],  # [m/s²]\n",
    "        \"speed_range\": [0, 15],  # [m/s]\n",
    "    },\n",
    "    \"simulation_frequency\": 15,\n",
    "    \"policy_frequency\":     5,\n",
    "    \"duration\": 300,\n",
    "    \"collision_reward\": -1,\n",
    "    \"lane_centering_cost\": 4,\n",
    "    \"action_reward\": -0.3,\n",
    "    \"controlled_vehicles\": 1,\n",
    "    \"other_vehicles\": 1,\n",
    "    \"screen_width\": 600,\n",
    "    \"screen_height\": 600,\n",
    "    \"centering_position\": [0.5, 0.5],\n",
    "    \"scaling\": 7,\n",
    "    \"show_trajectories\": False,\n",
    "    \"render_agent\": True,\n",
    "    \"offscreen_rendering\": False\n",
    "}\n",
    "\n",
    "\n",
    "env.unwrapped.configure(config)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learning with steering and acceleration\n",
    "TRAIN = True\n",
    "\n",
    "if True : \n",
    "    n_cpu = 6\n",
    "    batch_size = 64\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        policy_kwargs=dict(net_arch=[dict(pi=[256, 256], vf=[256, 256])]),\n",
    "        n_steps=batch_size * 12 // n_cpu,\n",
    "        batch_size=batch_size,\n",
    "        n_epochs=10,\n",
    "        learning_rate=5e-4,\n",
    "        gamma=0.9,\n",
    "        verbose=2,\n",
    "        tensorboard_log=\"racetrack_ppo/\",\n",
    "    )\n",
    "    # Train the model\n",
    "    if TRAIN:\n",
    "        model.learn(total_timesteps=int(1e5))\n",
    "        model.save(\"racetrack_ppo/model_sterring_acceleration\")\n",
    "        del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"racetrack-v0\", render_mode=\"rgb_array\")\n",
    "\n",
    "config = {\n",
    "    \"observation\": {\n",
    "        \"type\": \"OccupancyGrid\",\n",
    "        \"features\": [\"presence\", \"on_road\"],\n",
    "        \"grid_size\": [[-18, 18], [-18, 18]],\n",
    "        \"grid_step\": [5, 5],\n",
    "        \"as_image\": False,\n",
    "        \"align_to_vehicle_axes\": True\n",
    "    },\n",
    "    \"action\": {\n",
    "        \"type\": \"ContinuousAction\",\n",
    "        \"longitudinal\": True,\n",
    "        \"lateral\": True,\n",
    "        \"steering_range\": [-np.pi / 4, np.pi / 4],  # [rad]\n",
    "        \"acceleration_range\": [-2, 2],  # [m/s²]\n",
    "        \"speed_range\": [0, 15],  # [m/s]\n",
    "    },\n",
    "    \"simulation_frequency\": 15,\n",
    "    \"policy_frequency\": 5,\n",
    "    \"duration\": 300,\n",
    "    \"collision_reward\": -1,\n",
    "    \"lane_centering_cost\": 4,\n",
    "    \"action_reward\": -0.3,\n",
    "    \"controlled_vehicles\": 1,\n",
    "    \"other_vehicles\": 1,\n",
    "    \"screen_width\": 600,\n",
    "    \"screen_height\": 600,\n",
    "    \"centering_position\": [0.5, 0.5],\n",
    "    \"scaling\": 7,\n",
    "    \"show_trajectories\": False,\n",
    "    \"render_agent\": True,\n",
    "    \"offscreen_rendering\": False\n",
    "}\n",
    "\n",
    "\n",
    "env.unwrapped.configure(config)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML, display, clear_output\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "def run_one_episode_and_save(env, agent, to_display=True, save_video=True, max_steps=1000):\n",
    "    frames = []  \n",
    "    rewards = 0\n",
    "    step = 0\n",
    "    state_history = []\n",
    "    env.reset() \n",
    "    display_env = deepcopy(env)  \n",
    "\n",
    "    total_steps = 0\n",
    "    continue_loop = True\n",
    "    while continue_loop:\n",
    "        state, _ = display_env.reset()\n",
    "        done = False\n",
    "        additional_frames_when_done = 5\n",
    "        while not done:\n",
    "            clear_output(wait=True)\n",
    "            frame = display_env.render() \n",
    "            frames.append(frame)\n",
    "            if to_display:\n",
    "                plt.imshow(frame)\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "\n",
    "            action, _state = agent.predict(state, deterministic=True)\n",
    "            state, reward, episode_done, _, _ = display_env.step(action)\n",
    "            rewards += reward\n",
    "            step += 1\n",
    "            state_history.append(state)\n",
    "\n",
    "            total_steps += 1\n",
    "            if episode_done or step >= max_steps:\n",
    "                additional_frames_when_done -= 1\n",
    "                if additional_frames_when_done <= 0:\n",
    "                    done = True\n",
    "\n",
    "        if total_steps >= max_steps:\n",
    "            continue_loop = False\n",
    "\n",
    "    if save_video:\n",
    "        fig, ax = plt.subplots()\n",
    "        img_ax = ax.imshow(frames[0])  \n",
    "        ax.axis('off')  \n",
    "\n",
    "        def update(frame):\n",
    "            img_ax.set_data(frame)  \n",
    "\n",
    "        ani = FuncAnimation(fig, update, frames=frames, repeat=False)\n",
    "\n",
    "        video_filename = 'trained_agent_simulation_vCNN_4_bis_plus.mp4'\n",
    "        ani.save(video_filename, writer='ffmpeg', fps=50)\n",
    "\n",
    "        plt.close(fig)  \n",
    "\n",
    "    display_env.close()  \n",
    "\n",
    "    return np.array(state_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"model_sterring_acceleration\", env=env)\n",
    "\n",
    "state_history = run_one_episode_and_save(env, model, to_display=True, save_video=True, max_steps=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
