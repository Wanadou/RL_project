{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "import pprint\n",
    "import highway_env  # noqa: F401\n",
    "\n",
    "TRAIN = True\n",
    "\n",
    "n_cpu = 6\n",
    "batch_size = 64\n",
    "# env = make_vec_env(\"racetrack-v0\", n_envs=n_cpu, vec_env_cls=SubprocVecEnv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnvSpec(id='racetrack-v0', entry_point='highway_env.envs:RacetrackEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'render_mode': 'rgb_array'}, namespace=None, name='racetrack', version=0, additional_wrappers=(), vector_entry_point=None)\n",
      "{'action': {'lateral': True, 'longitudinal': False, 'type': 'ContinuousAction'},\n",
      " 'action_reward': -0.3,\n",
      " 'centering_position': [0.5, 0.5],\n",
      " 'collision_reward': -1,\n",
      " 'controlled_vehicles': 1,\n",
      " 'duration': 300,\n",
      " 'lane_centering_cost': 4,\n",
      " 'lane_centering_reward': 1,\n",
      " 'manual_control': False,\n",
      " 'observation': {'align_to_vehicle_axes': True,\n",
      "                 'as_image': False,\n",
      "                 'features': ['presence', 'on_road'],\n",
      "                 'grid_size': [[-18, 18], [-18, 18]],\n",
      "                 'grid_step': [3, 3],\n",
      "                 'type': 'OccupancyGrid'},\n",
      " 'offscreen_rendering': False,\n",
      " 'other_vehicles': 1,\n",
      " 'other_vehicles_type': 'highway_env.vehicle.behavior.IDMVehicle',\n",
      " 'policy_frequency': 5,\n",
      " 'real_time_rendering': False,\n",
      " 'render_agent': True,\n",
      " 'scaling': 7,\n",
      " 'screen_height': 600,\n",
      " 'screen_width': 600,\n",
      " 'show_trajectories': False,\n",
      " 'simulation_frequency': 15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/BOULLIERERWAN1/opt/anaconda3/envs/rl_py311/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.config to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.config` for environment variables or `env.get_wrapper_attr('config')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "from racetrack_config import *\n",
    "\n",
    "\n",
    "print(env.spec)\n",
    "pprint.pprint(env.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to racetrack_ppo/PPO_6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/BOULLIERERWAN1/opt/anaconda3/envs/rl_py311/lib/python3.11/site-packages/stable_baselines3/common/policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "| time/              |     |\n",
      "|    fps             | 20  |\n",
      "|    iterations      | 1   |\n",
      "|    time_elapsed    | 6   |\n",
      "|    total_timesteps | 128 |\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    policy_kwargs=dict(net_arch=[dict(pi=[256, 256], vf=[256, 256])]),\n",
    "    n_steps=batch_size * 12 // n_cpu,\n",
    "    batch_size=batch_size,\n",
    "    n_epochs=10,\n",
    "    learning_rate=5e-4,\n",
    "    gamma=0.9,\n",
    "    verbose=2,\n",
    "    tensorboard_log=\"racetrack_ppo/\",\n",
    ")\n",
    "# Train the model\n",
    "if TRAIN:\n",
    "    model.learn(total_timesteps=int(1e2)) # try with 1e5 but takes a very long time!!\n",
    "    model.save(\"racetrack_ppo/model\")\n",
    "    # del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vizualise in a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Render mode is None, which is incompatible with RecordVideo. Initialize your environment with a render_mode that returns an image, such as rgb_array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mracetrack_ppo/model\u001b[39m\u001b[38;5;124m\"\u001b[39m, env\u001b[38;5;241m=\u001b[39menv)\n\u001b[1;32m      4\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mracetrack-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mRecordVideo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mracetrack_ppo/videos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_trigger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m env\u001b[38;5;241m.\u001b[39munwrapped\u001b[38;5;241m.\u001b[39mset_record_video_wrapper(env)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rl_py311/lib/python3.11/site-packages/gymnasium/wrappers/record_video.py:74\u001b[0m, in \u001b[0;36mRecordVideo.__init__\u001b[0;34m(self, env, video_folder, episode_trigger, step_trigger, video_length, name_prefix, disable_logger)\u001b[0m\n\u001b[1;32m     71\u001b[0m gym\u001b[38;5;241m.\u001b[39mWrapper\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mansi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mansi_list\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRender mode is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39mrender_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, which is incompatible with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m RecordVideo. Initialize your environment with a render_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m that returns an image, such as rgb_array.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     78\u001b[0m     )\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m episode_trigger \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m step_trigger \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     episode_trigger \u001b[38;5;241m=\u001b[39m capped_cubic_video_schedule\n",
      "\u001b[0;31mValueError\u001b[0m: Render mode is None, which is incompatible with RecordVideo. Initialize your environment with a render_mode that returns an image, such as rgb_array."
     ]
    }
   ],
   "source": [
    "# Run the algorithm\n",
    "model = PPO.load(\"racetrack_ppo/model\", env=env)\n",
    "\n",
    "env = gym.make(\"racetrack-v0\")\n",
    "env = RecordVideo(\n",
    "    env, video_folder=\"racetrack_ppo/videos\", episode_trigger=lambda e: True\n",
    ")\n",
    "env.unwrapped.set_record_video_wrapper(env)\n",
    "\n",
    "for video in range(100):\n",
    "    done = truncated = False\n",
    "    obs, info = env.reset()\n",
    "    while not (done or truncated):\n",
    "        # Predict\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        # Get reward\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        # Render\n",
    "        env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
